{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "bootstrap.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salmanahmad10/BBAK/blob/main/version3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCp8kn3-vnqe",
        "outputId": "c101f01a-4d9d-471c-92a4-5f8814c679f7"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')\r\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br7eQmjBu2-W"
      },
      "source": [
        "from os import listdir\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer, one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model, Sequential, model_from_json,save_model\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers.core import Dense, Dropout, Flatten\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Embedding, TimeDistributed, RepeatVector, LSTM, concatenate , Input, Reshape, Dense\n",
        "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
        "import numpy as np\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4NpvoN_u2-Z"
      },
      "source": [
        "dir_name = 'gdrive/My Drive/SharedGoogleDrive/Dataset/pix2code_datasets/web/eval_light/'\n",
        "\n",
        "# Read a file and return a string\n",
        "def load_doc(filename):\n",
        "    file = open(filename, 'r')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "def load_data(data_dir):\n",
        "    text = []\n",
        "    images = []\n",
        "    # Load all the files and order them\n",
        "    all_filenames = listdir(data_dir)\n",
        "    all_filenames.sort()\n",
        "    for filename in (all_filenames):\n",
        "        if filename[-3:] == \"npz\":\n",
        "            # Load the images already prepared in arrays\n",
        "            image = np.load(data_dir+filename)\n",
        "            images.append(image['features'])\n",
        "        else:\n",
        "            # Load the boostrap tokens and rap them in a start and end tag\n",
        "            syntax = '<START> ' + load_doc(data_dir+filename) + ' <END>'\n",
        "            # Seperate all the words with a single space\n",
        "            syntax = ' '.join(syntax.split())\n",
        "            # Add a space after each comma\n",
        "            syntax = syntax.replace(',', ' ,')\n",
        "            text.append(syntax)\n",
        "    images = np.array(images, dtype=float)\n",
        "    return images, text\n",
        "\n",
        "train_features, texts = load_data(dir_name)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtNlyWHFu2-Z"
      },
      "source": [
        "# Initialize the function to create the vocabulary \n",
        "tokenizer = Tokenizer(filters='', split=\" \", lower=False)\n",
        "# Create the vocabulary \n",
        "tokenizer.fit_on_texts([load_doc('gdrive/My Drive/SharedGoogleDrive/Dataset/pix2code_datasets/web/bootstrap.vocab')])\n",
        "\n",
        "# Add one spot for the empty word in the vocabulary \n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "# Map the input sentences into the vocabulary indexes\n",
        "train_sequences = tokenizer.texts_to_sequences(texts)\n",
        "# The longest set of boostrap tokens\n",
        "max_sequence = max(len(s) for s in train_sequences)\n",
        "# Specify how many tokens to have in each input sentence\n",
        "max_length = 48\n",
        "\n",
        "def preprocess_data(sequences, features):\n",
        "    X, y, image_data = list(), list(), list()\n",
        "    for img_no, seq in enumerate(sequences):\n",
        "        for i in range(1, len(seq)):\n",
        "            # Add the sentence until the current count(i) and add the current count to the output\n",
        "            in_seq, out_seq = seq[:i], seq[i]\n",
        "            # Pad all the input token sentences to max_sequence\n",
        "            in_seq = pad_sequences([in_seq], maxlen=max_sequence)[0]\n",
        "            # Turn the output into one-hot encoding\n",
        "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "            # Add the corresponding image to the boostrap token file\n",
        "            image_data.append(features[img_no])\n",
        "            # Cap the input sentence to 48 tokens and add it\n",
        "            X.append(in_seq[-48:])\n",
        "            y.append(out_seq)\n",
        "    return np.array(X), np.array(y), np.array(image_data)\n",
        "\n",
        "X, y, image_data = preprocess_data(train_sequences, train_features)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZY2lfvobu2-a"
      },
      "source": [
        "#Create the encoder\n",
        "image_model = Sequential()\n",
        "image_model.add(Conv2D(16, (3, 3), padding='valid', activation='relu', input_shape=(256, 256, 3,)))\n",
        "image_model.add(Conv2D(16, (3,3), activation='relu', padding='same', strides=2))\n",
        "image_model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
        "image_model.add(Conv2D(32, (3,3), activation='relu', padding='same', strides=2))\n",
        "image_model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
        "image_model.add(Conv2D(64, (3,3), activation='relu', padding='same', strides=2))\n",
        "image_model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
        "\n",
        "image_model.add(Flatten())\n",
        "image_model.add(Dense(1024, activation='relu'))\n",
        "image_model.add(Dropout(0.3))\n",
        "image_model.add(Dense(1024, activation='relu'))\n",
        "image_model.add(Dropout(0.3))\n",
        "\n",
        "image_model.add(RepeatVector(max_length))\n",
        "\n",
        "visual_input = Input(shape=(256, 256, 3,))\n",
        "encoded_image = image_model(visual_input)\n",
        "\n",
        "language_input = Input(shape=(max_length,))\n",
        "language_model = Embedding(vocab_size, 50, input_length=max_length, mask_zero=True)(language_input)\n",
        "language_model = LSTM(128, return_sequences=True)(language_model)\n",
        "language_model = LSTM(128, return_sequences=True)(language_model)\n",
        "\n",
        "#Create the decoder\n",
        "decoder = concatenate([encoded_image, language_model])\n",
        "decoder = LSTM(512, return_sequences=True)(decoder)\n",
        "decoder = LSTM(512, return_sequences=False)(decoder)\n",
        "decoder = Dense(vocab_size, activation='softmax')(decoder)\n",
        "\n",
        "# Compile the model\n",
        "model = Model(inputs=[visual_input, language_input], outputs=decoder)\n",
        "optimizer = RMSprop(lr=0.0001, clipvalue=1.0)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwZYdnlWu2-a",
        "outputId": "4b4dbcc3-0ee8-43cf-e2c1-b2c1591c705c"
      },
      "source": [
        "#Save the model for every 2nd epoch\n",
        "filepath=\"org-weights-epoch-{epoch:04d}--val_loss-{val_loss:.4f}--loss-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_weights_only=True, period=2)\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6R-C0SAFu2-b",
        "outputId": "a386efd5-f7ab-42a6-be22-290125f02ca7"
      },
      "source": [
        "# Train the model\n",
        "model.fit([image_data, X], y, batch_size=1, shuffle=False, validation_split=0.1, callbacks=callbacks_list, verbose=1, epochs=50)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "624/624 [==============================] - 140s 200ms/step - loss: 2.6320 - val_loss: 2.3853\n",
            "Epoch 2/50\n",
            "624/624 [==============================] - 121s 194ms/step - loss: 2.4122 - val_loss: 2.3235\n",
            "\n",
            "Epoch 00002: saving model to org-weights-epoch-0002--val_loss-2.3235--loss-2.3872.hdf5\n",
            "Epoch 3/50\n",
            "624/624 [==============================] - 120s 193ms/step - loss: 2.2631 - val_loss: 2.2443\n",
            "Epoch 4/50\n",
            "624/624 [==============================] - 121s 195ms/step - loss: 1.6404 - val_loss: 3.2347\n",
            "\n",
            "Epoch 00004: saving model to org-weights-epoch-0004--val_loss-3.2347--loss-1.5374.hdf5\n",
            "Epoch 5/50\n",
            "624/624 [==============================] - 124s 198ms/step - loss: 1.4969 - val_loss: 3.4441\n",
            "Epoch 6/50\n",
            "624/624 [==============================] - 121s 194ms/step - loss: 1.3839 - val_loss: 2.5603\n",
            "\n",
            "Epoch 00006: saving model to org-weights-epoch-0006--val_loss-2.5603--loss-1.3035.hdf5\n",
            "Epoch 7/50\n",
            "624/624 [==============================] - 123s 197ms/step - loss: 1.2434 - val_loss: 2.7765\n",
            "Epoch 8/50\n",
            "624/624 [==============================] - 122s 195ms/step - loss: 1.2256 - val_loss: 2.8252\n",
            "\n",
            "Epoch 00008: saving model to org-weights-epoch-0008--val_loss-2.8252--loss-1.1044.hdf5\n",
            "Epoch 9/50\n",
            "624/624 [==============================] - 121s 194ms/step - loss: 1.2687 - val_loss: 2.3166\n",
            "Epoch 10/50\n",
            "624/624 [==============================] - 121s 194ms/step - loss: 1.0751 - val_loss: 2.4076\n",
            "\n",
            "Epoch 00010: saving model to org-weights-epoch-0010--val_loss-2.4076--loss-0.9630.hdf5\n",
            "Epoch 11/50\n",
            "624/624 [==============================] - 122s 196ms/step - loss: 0.9952 - val_loss: 2.7995\n",
            "Epoch 12/50\n",
            "624/624 [==============================] - 121s 194ms/step - loss: 0.9489 - val_loss: 2.4824\n",
            "\n",
            "Epoch 00012: saving model to org-weights-epoch-0012--val_loss-2.4824--loss-0.8643.hdf5\n",
            "Epoch 13/50\n",
            "624/624 [==============================] - 121s 194ms/step - loss: 0.8249 - val_loss: 2.9369\n",
            "Epoch 14/50\n",
            "624/624 [==============================] - 122s 195ms/step - loss: 0.8164 - val_loss: 3.2071\n",
            "\n",
            "Epoch 00014: saving model to org-weights-epoch-0014--val_loss-3.2071--loss-0.7590.hdf5\n",
            "Epoch 15/50\n",
            "624/624 [==============================] - 122s 195ms/step - loss: 0.7641 - val_loss: 2.3608\n",
            "Epoch 16/50\n",
            "624/624 [==============================] - 121s 194ms/step - loss: 0.9246 - val_loss: 2.3063\n",
            "\n",
            "Epoch 00016: saving model to org-weights-epoch-0016--val_loss-2.3063--loss-0.8614.hdf5\n",
            "Epoch 17/50\n",
            "624/624 [==============================] - 123s 197ms/step - loss: 0.6301 - val_loss: 2.1226\n",
            "Epoch 18/50\n",
            "624/624 [==============================] - 121s 194ms/step - loss: 0.6973 - val_loss: 1.3529\n",
            "\n",
            "Epoch 00018: saving model to org-weights-epoch-0018--val_loss-1.3529--loss-0.6170.hdf5\n",
            "Epoch 19/50\n",
            "624/624 [==============================] - 123s 197ms/step - loss: 0.5587 - val_loss: 2.0005\n",
            "Epoch 20/50\n",
            "624/624 [==============================] - 121s 194ms/step - loss: 0.6048 - val_loss: 1.2981\n",
            "\n",
            "Epoch 00020: saving model to org-weights-epoch-0020--val_loss-1.2981--loss-0.5409.hdf5\n",
            "Epoch 21/50\n",
            "624/624 [==============================] - 123s 197ms/step - loss: 0.5113 - val_loss: 1.4691\n",
            "Epoch 22/50\n",
            "624/624 [==============================] - 122s 196ms/step - loss: 0.4473 - val_loss: 2.0983\n",
            "\n",
            "Epoch 00022: saving model to org-weights-epoch-0022--val_loss-2.0983--loss-0.4418.hdf5\n",
            "Epoch 23/50\n",
            "624/624 [==============================] - 122s 195ms/step - loss: 0.4372 - val_loss: 0.4913\n",
            "Epoch 24/50\n",
            "624/624 [==============================] - 123s 197ms/step - loss: 0.3766 - val_loss: 1.8072\n",
            "\n",
            "Epoch 00024: saving model to org-weights-epoch-0024--val_loss-1.8072--loss-0.3943.hdf5\n",
            "Epoch 25/50\n",
            "624/624 [==============================] - 122s 195ms/step - loss: 0.3738 - val_loss: 1.0432\n",
            "Epoch 26/50\n",
            "624/624 [==============================] - 120s 193ms/step - loss: 0.3474 - val_loss: 2.0160\n",
            "\n",
            "Epoch 00026: saving model to org-weights-epoch-0026--val_loss-2.0160--loss-0.3713.hdf5\n",
            "Epoch 27/50\n",
            "624/624 [==============================] - 121s 194ms/step - loss: 0.3634 - val_loss: 1.0036\n",
            "Epoch 28/50\n",
            "624/624 [==============================] - 121s 193ms/step - loss: 0.3605 - val_loss: 1.9703\n",
            "\n",
            "Epoch 00028: saving model to org-weights-epoch-0028--val_loss-1.9703--loss-0.3618.hdf5\n",
            "Epoch 29/50\n",
            "624/624 [==============================] - 123s 197ms/step - loss: 0.3474 - val_loss: 1.6686\n",
            "Epoch 30/50\n",
            "624/624 [==============================] - 122s 195ms/step - loss: 0.3332 - val_loss: 2.0515\n",
            "\n",
            "Epoch 00030: saving model to org-weights-epoch-0030--val_loss-2.0515--loss-0.3517.hdf5\n",
            "Epoch 31/50\n",
            "624/624 [==============================] - 122s 195ms/step - loss: 0.3314 - val_loss: 2.0941\n",
            "Epoch 32/50\n",
            "624/624 [==============================] - 123s 196ms/step - loss: 0.3334 - val_loss: 2.0855\n",
            "\n",
            "Epoch 00032: saving model to org-weights-epoch-0032--val_loss-2.0855--loss-0.3139.hdf5\n",
            "Epoch 33/50\n",
            "624/624 [==============================] - 121s 194ms/step - loss: 0.3405 - val_loss: 2.1676\n",
            "Epoch 34/50\n",
            "624/624 [==============================] - 121s 194ms/step - loss: 0.3362 - val_loss: 2.2310\n",
            "\n",
            "Epoch 00034: saving model to org-weights-epoch-0034--val_loss-2.2310--loss-0.3117.hdf5\n",
            "Epoch 35/50\n",
            "624/624 [==============================] - 121s 195ms/step - loss: 0.3005 - val_loss: 1.9957\n",
            "Epoch 36/50\n",
            "624/624 [==============================] - 122s 195ms/step - loss: 0.2998 - val_loss: 2.3624\n",
            "\n",
            "Epoch 00036: saving model to org-weights-epoch-0036--val_loss-2.3624--loss-0.2864.hdf5\n",
            "Epoch 37/50\n",
            "624/624 [==============================] - 123s 197ms/step - loss: 0.3260 - val_loss: 1.4022\n",
            "Epoch 38/50\n",
            "624/624 [==============================] - 121s 194ms/step - loss: 0.3724 - val_loss: 1.1170\n",
            "\n",
            "Epoch 00038: saving model to org-weights-epoch-0038--val_loss-1.1170--loss-0.3238.hdf5\n",
            "Epoch 39/50\n",
            "624/624 [==============================] - 122s 196ms/step - loss: 0.3388 - val_loss: 3.9514\n",
            "Epoch 40/50\n",
            "624/624 [==============================] - 122s 196ms/step - loss: 0.3198 - val_loss: 1.2905\n",
            "\n",
            "Epoch 00040: saving model to org-weights-epoch-0040--val_loss-1.2905--loss-0.3023.hdf5\n",
            "Epoch 41/50\n",
            "624/624 [==============================] - 122s 196ms/step - loss: 0.2933 - val_loss: 1.3977\n",
            "Epoch 42/50\n",
            "624/624 [==============================] - 123s 196ms/step - loss: 0.2722 - val_loss: 1.6855\n",
            "\n",
            "Epoch 00042: saving model to org-weights-epoch-0042--val_loss-1.6855--loss-0.2483.hdf5\n",
            "Epoch 43/50\n",
            "624/624 [==============================] - 123s 197ms/step - loss: 0.2414 - val_loss: 1.8073\n",
            "Epoch 44/50\n",
            "624/624 [==============================] - 121s 193ms/step - loss: 0.2988 - val_loss: 1.0503\n",
            "\n",
            "Epoch 00044: saving model to org-weights-epoch-0044--val_loss-1.0503--loss-0.2462.hdf5\n",
            "Epoch 45/50\n",
            "624/624 [==============================] - 121s 193ms/step - loss: 0.2626 - val_loss: 1.3931\n",
            "Epoch 46/50\n",
            "624/624 [==============================] - 122s 196ms/step - loss: 0.2494 - val_loss: 0.8783\n",
            "\n",
            "Epoch 00046: saving model to org-weights-epoch-0046--val_loss-0.8783--loss-0.2044.hdf5\n",
            "Epoch 47/50\n",
            "624/624 [==============================] - 122s 195ms/step - loss: 0.3138 - val_loss: 1.7136\n",
            "Epoch 48/50\n",
            "624/624 [==============================] - 122s 196ms/step - loss: 0.2267 - val_loss: 1.2594\n",
            "\n",
            "Epoch 00048: saving model to org-weights-epoch-0048--val_loss-1.2594--loss-0.1721.hdf5\n",
            "Epoch 49/50\n",
            "624/624 [==============================] - 123s 197ms/step - loss: 0.2498 - val_loss: 1.3275\n",
            "Epoch 50/50\n",
            "624/624 [==============================] - 122s 195ms/step - loss: 0.2066 - val_loss: 0.5221\n",
            "\n",
            "Epoch 00050: saving model to org-weights-epoch-0050--val_loss-0.5221--loss-0.1590.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8ca33c0da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boEYZob4u2-c"
      },
      "source": [
        "model.save('gdrive/My Drive/SharedGoogleDrive/Dataset/pix2code_datasets/model.h5')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7f3jhX-Z7-b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}